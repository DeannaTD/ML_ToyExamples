{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1. Removing unanswerable QA pairs\n",
    "Check back Tutorial 2 on how to fix a specific random seed for reproducibility!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch as tc\n",
    "import random\n",
    "import sklearn as sk\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "import transformers\n",
    "from transformers import EncoderDecoderModel, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code provided in assignment docs\n",
    "#Dataset download\n",
    "import os\n",
    "import urllib.request\n",
    "from tqdm import tqdm\n",
    "\n",
    "class DownloadProgressBar(tqdm):\n",
    "    def update_to(self, b=1, bsize=1, tsize=None):\n",
    "        if tsize is not None:\n",
    "            self.total = tsize\n",
    "        self.update(b * bsize - self.n)\n",
    "        \n",
    "def download_url(url, output_path):\n",
    "    with DownloadProgressBar(unit='B', unit_scale=True,\n",
    "                             miniters=1, desc=url.split('/')[-1]) as t:\n",
    "        urllib.request.urlretrieve(url, filename=output_path, reporthook=t.update_to)\n",
    "\n",
    "def download_data(data_path, url_path, suffix):    \n",
    "    if not os.path.exists(data_path):\n",
    "        os.makedirs(data_path)\n",
    "        \n",
    "    data_path = os.path.join(data_path, f'{suffix}.json')\n",
    "    if not os.path.exists(data_path):\n",
    "        print(f\"Downloading CoQA {suffix} data split... (it may take a while)\")\n",
    "        download_url(url=url_path, output_path=data_path)\n",
    "        print(\"Download completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and test data loading\n",
    "train_url = \"https://nlp.stanford.edu/data/coqa/coqa-train-v1.0.json\"\n",
    "download_data(data_path='coqa', url_path=train_url, suffix='train')\n",
    "\n",
    "test_url = \"https://nlp.stanford.edu/data/coqa/coqa-dev-v1.0.json\"\n",
    "download_data(data_path='coqa', url_path=test_url, suffix='test')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading data from JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading train.json to dataframe\n",
    "train_df = pd.read_json('coqa/train.json')\n",
    "\n",
    "#deleting the \"version\" json property\n",
    "del train_df[\"version\"]\n",
    "\n",
    "cols = [\"source\", \"id\", \"filename\", \"story\", \"question\", \"qturn_id\", \"span_start\",\"span_end\", \"span_text\", \"answer\", \"aturn_id\"]\n",
    "comp_list = []\n",
    "for index, row in train_df.iterrows():\n",
    "   for i in range(len(row[\"data\"][\"questions\"])):\n",
    "        row_insert = []\n",
    "        row_insert.append(row[\"data\"][\"source\"])\n",
    "        row_insert.append(row[\"data\"][\"id\"])\n",
    "        row_insert.append(row[\"data\"][\"filename\"])\n",
    "        row_insert.append(row[\"data\"][\"story\"])\n",
    "        for key in row[\"data\"][\"questions\"][i]:\n",
    "            row_insert.append(row[\"data\"][\"questions\"][i][key])\n",
    "        for key in row[\"data\"][\"answers\"][i]:\n",
    "            row_insert.append(row[\"data\"][\"answers\"][i][key])\n",
    "        #truncating bad_turn property if presented\n",
    "        comp_list.append(row_insert[:11])\n",
    "\n",
    "train_df = pd.DataFrame(comp_list, columns=cols)\n",
    "#droping the unanswerable QA pairs\n",
    "train_df = train_df[train_df[\"answer\"] != \"unknown\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learnNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "  Downloading scikit_learn-1.2.0-cp310-cp310-win_amd64.whl (8.2 MB)\n",
      "     ---------------------------------------- 8.2/8.2 MB 223.9 kB/s eta 0:00:00\n",
      "Collecting joblib>=1.1.1\n",
      "  Using cached joblib-1.2.0-py3-none-any.whl (297 kB)\n",
      "Collecting scipy>=1.3.2\n",
      "  Downloading scipy-1.9.3-cp310-cp310-win_amd64.whl (40.1 MB)\n",
      "     -------------------------------------- 40.1/40.1 MB 252.9 kB/s eta 0:00:00\n",
      "Requirement already satisfied: numpy>=1.17.3 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn) (1.23.0)\n",
      "Collecting threadpoolctl>=2.0.0\n",
      "  Downloading threadpoolctl-3.1.0-py3-none-any.whl (14 kB)\n",
      "Installing collected packages: threadpoolctl, scipy, joblib, scikit-learn\n",
      "Successfully installed joblib-1.2.0 scikit-learn-1.2.0 scipy-1.9.3 threadpoolctl-3.1.0\n"
     ]
    }
   ],
   "source": [
    "%pip install scikit-learn"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the train set into train and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating the group splitter\n",
    "spl = GroupShuffleSplit(n_splits=2, test_size=0.2, random_state=42)\n",
    "split = spl.split(train_df, groups=train_df[\"story\"])\n",
    "\n",
    "train_inds, val_inds = next(split)\n",
    "\n",
    "train = train_df.iloc[train_inds]\n",
    "validation = train_df.iloc[val_inds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model definition - DistilRoBERTa and BERTTiny"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drbt = EncoderDecoderModel.from_encoder_decoder_pretrained(\"distilroberta-base\", \"distilroberta-base\")\n",
    "berttiny = EncoderDecoderModel.from_encoder_decoder_pretrained(\"prajjwal1/bert-tiny\", \"prajjwal1/bert-tiny\")\n",
    "bert_tokenizer = AutoTokenizer.from_pretrained(\"prajjwal1/bert-tiny\")\n",
    "drbt_tokenizer = AutoTokenizer.from_pretrained( \"distilroberta-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_tokens(tokenizer, train_set):\n",
    "    question = train_set[\"question\"][:10000].tolist()\n",
    "    story = train_set[\"story\"][:10000].tolist()\n",
    "    inputs = tokenizer(question,\n",
    "                       story,\n",
    "                       truncation=True,\n",
    "                       padding=True,\n",
    "                       max_length=512,\n",
    "                       return_tensors=\"pt\")\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_21148\\2008519375.py:2: FutureWarning: The behavior of `series[i:j]` with an integer-dtype index is deprecated. In a future version, this will be treated as *label-based* indexing, consistent with e.g. `series[i]` lookups. To retain the old behavior, use `series.iloc[i:j]`. To get the future behavior, use `series.loc[i:j]`.\n",
      "  question = train_set[\"question\"][:10000].tolist()\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_21148\\2008519375.py:3: FutureWarning: The behavior of `series[i:j]` with an integer-dtype index is deprecated. In a future version, this will be treated as *label-based* indexing, consistent with e.g. `series[i]` lookups. To retain the old behavior, use `series.iloc[i:j]`. To get the future behavior, use `series.loc[i:j]`.\n",
      "  story = train_set[\"story\"][:10000].tolist()\n"
     ]
    }
   ],
   "source": [
    "bt_inputs = input_tokens(bert_tokenizer, train)\n",
    "dt_inputs = input_tokens(drbt_tokenizer, train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = bt_inputs[\"input_ids\"]\n",
    "labels = bt_inputs[\"input_ids\"]\n",
    "\n",
    "loss = berttiny(input_ids = input_ids, decoder_input_ids=labels, labels=labels).loss\n",
    "loss.backward()\n",
    "\n",
    "berttiny.eval()\n",
    "\n",
    "\"\"\"\n",
    "input_ids = tokenizer('example question', return_tensors='pt').input_ids\n",
    "labels = tokenizer('example answerr', return_tensors='pt').input_ids\n",
    "\n",
    "loss = bert2bert(input_ids=input_ids, decoder_input_ids=labels, labels=labels).loss\n",
    "loss.backward()\n",
    "\n",
    "bert2bert.eval()\n",
    "greedy_output = bert2bert.generate(input_ids, decoder_start_token_id=bert2bert.config.decoder.pad_token_id)\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "afb734500600fd355917ca529030176ea0ca205570884b88f2f6f7d791fd3fbe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
